---
title: CNN笔记
key: 20180724
tags: DeepLearning
---

## 流程

![](https://cl.ly/3W3N002i403j/Image%202018-07-03%20at%203.27.27%20PM.png)

L 层神经网络， forward propagation
![](https://cl.ly/1q042z3s1V2r/Image%202018-07-03%20at%203.28.29%20PM.png)

```python
Z, cache = linear_forward(A, W, b) # cache = (A, W, b) 

parameters = initialize_parameters_deep(layer_dims)

Z, cache = linear_forward(A, W, b)

A, cache = linear_activation_forward(A_prev, W, b, activation)

AL, caches = L_model_forward(X, parameters)

cost = compute_cost(AL, Y)
```
![](https://cl.ly/3u0c1V2m0H0I/Image%202018-07-03%20at%204.03.43%20PM.png)

##  L-Model Backward
Now you will implement the backward function for the whole network. Recall that when you implemented the L_model_forward function, at each iteration, you stored a cache which contains (X,W,b, and z). 

In the back propagation module, you will use those variables to compute the gradients. Therefore, in the L_model_backward function, you will iterate through all the hidden layers backward, starting from layer  LL . On each step, you will use the cached values for layer  ll  to backpropagate through layer  ll . Figure 5 below shows the backward pass.

## 范数
降低过拟合程度：
正则化之所以能够降低过拟合的原因在于，正则化是结构风险最小化的一种策略实现。给loss function加上正则化项，能使得新得到的优化目标函数h = f+normal，需要在f和normal中做一个权衡（trade-off），如果还像原来只优化f的情况下，那可能得到一组解比较复杂，使得正则项normal比较大，那么h就不是最优的，因此可以看出加正则项能让解更加简单，符合奥卡姆剃刀理论，同时也比较符合在偏差和方差（方差表示模型的复杂度）分析中，通过降低模型复杂度，得到更小的泛化误差，降低过拟合程度。

L1正则化和L2正则化：
L1正则化就是在loss function后边所加正则项为L1范数，加上L1范数容易得到稀疏解（0比较多）。L2正则化就是loss function后边所加正则项为L2范数的平方，加上L2正则相比于L1正则来说，得到的解比较平滑（不是稀疏），但是同样能够保证解中接近于0（但不是等于0，所以相对平滑）的维度比较多，降低模型的复杂度。

## CNN
整个网络的流程
![](https://cl.ly/3N2k311U0v3A/Image%202018-07-04%20at%204.20.40%20PM.png)

### Convolution
把图像与filter做卷积，得到feature map
CNN等于是把全连去掉一些weight
<div align="center">
<img src="https://cl.ly/0G312E0l0C29/Image%202018-07-04%20at%204.10.48%20PM.png" height="300px" alt="yoyoyo" >
<img src="https://cl.ly/3I1R3k0v1p0k/Image%202018-07-04%20at%204.12.38%20PM.png" height="300px" alt="图片说明" >
</div>

### Max pooling
图像划分成2*2的小块，每块只保留最大值。
<div align="center">
<img src="https://cl.ly/2T3M0S3J003W/Image%202018-07-04%20at%204.16.18%20PM.png" height="300px" alt="图片说明" >
<img src="https://cl.ly/2B3v2c3u0K35/Image%202018-07-04%20at%204.15.38%20PM.png" height="300px" alt="图片说明" >
</div>

## Convolutions over volume
对于RGB图片，对不同层分别设置filter值可以检测不同颜色的边界。
<div align="center">
<img src="https://cl.ly/0P0H2c1N3X3P/Image%202018-07-16%20at%204.01.10%20PM.png" height="300px" alt="图片说明" >
<img src="https://cl.ly/333M2Y2D2g3Q/Image%202018-07-16%20at%204.09.28%20PM.png" height="300px" alt="图片说明" >
</div>

## CNN paper
ImageNet LSVRC-2010 contest into the 1000 dif-ferent classes. 
60 million parameters and 650,000 neurons. 

Therefore, we down-sampled the images to a fixed resolution of 256  256. Given a rectangular image, we first rescaled the image such that the shorter side was of length 256, and then cropped out the central256 256patch from the resulting image. 

然后从每个pixel里面减去真个数据集的overall mean.
![](https://cl.ly/0P3u430R2Y2I/Image%202018-07-17%20at%209.47.09%20AM.png)

根据重要性，影响网络性能的因素有： 
* 1.ReLU Nonlinearit
* 2.Training on Multiple GPUs 
* 3.Local Response Normalization
* 4.Overlapping Pooling

### ReLU Nonlinearit
* ReLUs train several times faster than their equivalents with tanh units
* 饱和非线性(saturating nonlinearities，tanh)比不饱和非线性(ReLU)慢得多
![](https://ws4.sinaimg.cn/large/006tKfTcgy1ftcmp1h17aj30b3092glq.jpg)
* A four-layer convolutional neural network with ReLUs(solid line)reaches a 25% training error rate on CIFAR-10 six times faster than an equivalent network with tanh neurons (dashed line).

### Training on Multiple GPUs 
* cross-GPU parallelization, 不必走内存
* 把neurons分散在不同gpu，只在某些层communicate
    * 3 take input from all kernel maps in layer 2
    * layer 4 take input only from those kernel maps in layer 3 which reside on the same GPU

### Local Response Normalization
* ReLUs do not require input normalization to prevent them from saturating

### Overlapping Pooling 
* 池化层-间隔s像素的池化单元网格
