---
title: NIS debug 笔记
key: 20180922
tags: 法器集合
---

## Linux Bridge and Virtual Networking
[Virtual networking](https://cloudbuilder.in/blogs/2012/08/16/understanding-virtual-networks-the-basics/) requires the presence of a virtual switch inside a server/hypervisor. Even though it is called a bridge, the Linux bridge is really a virtual switch and used with KVM/QEMU hypervisor. Linux Bridge is a kernel module, first introduced in 2.2 kernel (circa 2000). And it is administered using brctl command on Linux.

### [容器是怎么连接到外面的网络的？](https://cloudbuilder.in/blogs/2013/12/02/linux-bridge-virtual-networking/)
![](https://cl.ly/413602f0d566/download/Image%2525202018-09-23%252520at%25252010.46.02%252520PM.png)
默认情况下，容器内部能够访问外网（当然你本身机器要联通外网）。这个是怎么做到的呢？
每创建一个容器，docker 会新建一对 interfaces，这对 interfaces 最大的特性就是：从一个地方进去的网络报文都能在另外一个接口被接受，就像水管的两头。

一个接口命名为 eth0，分配给容器内部；另外一个接口命名是 veth** 这样的形式，显示在 host 机器上，连接到 docker0。

外部是无法感知到容器内部的网络的，或者说容器的网络是被主机 eth0 网络封装起来的。那么一个自然的问题就是：容器怎么提供服务供外部访问？

答案只有四个字：端口映射。

## nis 工作原理

### story before RPC
The RPC portmapper (portmap(8)) is a server that converts RPC program numbers into TCP/IP (or UDP/IP) protocol port numbers. 

It must be running in order to make RPC calls (which is what the NIS/NIS+ client software does) to RPC servers (like a NIS or NIS+ server) on that machine. When an RPC server is started, it will tell portmap what port number it is listening to, and what RPC program numbers it is prepared to serve. -- 就是一个服务注册/服务发现的过程。

When a client wishes to make an RPC call to a given program number, it will first contact portmap on the server machine to determine the port number where RPC packets should be sent.

总结就是 portmapper(运行在 rpc server 的111端口) 帮 rpc 进行 program number 与 port number 之间的转换。


### About NIS server

* NIS 服务需要/提供的数据：

| 服务器端文件名 | 档案内容 |
| :---------- | :------ |
/etc/passwd | 提供用户账号、UID、GID、家目录所在、Shell 等等
/etc/group | 提供群组数据以及 GID 的对应，还有该群组的加入人员
/etc/hosts | 主机名与 IP 的对应，常用于 private IP 的主机名对应
/etc/services | 每一种服务 (daemons) 所对应的埠口 (port number)
/etc/protocols | 基础的 TCP/IP 封包协定，如 TCP, UDP, ICMP 等
/etc/rpc | 每种 RPC 服务器所对应的程序号码
/var/yp/ypservers | NIS 服务器所提供的数据库

* NIS server/ client 运作与查询
![](https://cl.ly/c427d438ebc1/Image%2525202018-09-28%252520at%2525203.31.43%252520PM.png)

**关于 NIS Server (master/slave) 的运作程序：**
* NIS Master 先将本身的账号密码相关档案制作成为数据库档案；
* NIS Master 可以主动的告知 NIS slave server 来更新；
* NIS slave 亦可主动的前往 NIS master server 取得更新后的数据库档案；
* 若有账号密码的异动时，需要重新制作 database 与重新同步化 master/slave。

**关于当 NIS Client 有任何登入查询的需求时：**
* NIS client 若有登入需求时，会先查询其本机的 /etc/passwd, /etc/shadow 等档案；
* 若在 NIS Client 本机找不到相关的账号数据，才开始向整个 NIS 网域的主机广播查询；
* 每部 NIS server (不论 master/slave) 都可以响应，基本上是『先响应者优先』。

#### 所需要的软件 & 配置



### [ypbind](https://www.systutorials.com/docs/linux/man/8-ypbind/)

ypbind finds the server for NIS domains and maintains the NIS binding information. The client (normaly the NIS routines in the standard C library) could get the information over RPC from ypbind or read the binding files. -- client 通过 ypbind 发现并使用 nis 服务。

The binding files resides in the directory **/var/yp/binding** and are conventionally named [domainname].[version]. The supported versions are 1 and 2. There could be several such files since it is possible for an NIS client to be bound to more than one domain.

ypbind 会用 1、广播寻找 nis server；2、根据 given list of known secure servers 寻找 (send a ping to all servers and binds to first one which answers)。



* /var/yp 下：ypbind 的信息
* /usr/lib/yp 下：yp 相关命令工具

#### 由ypbind开始的debug思路：
* nis client 下的 /var/yp/ 目录下应该有什么东西，都有何作用。
* k8s 里的 client 缺少什么配置？为什么缺少？


根据此信息结合上面在k8s下的slurm-client 执行 yptest的返回值：Test 2: ypbind ：Can't communicate with ypbind.

在docker上的slurm client中：
```bash
root@slurm-client:/# rpcinfo -p localhost
   program vers proto   port  service
    100000    4   tcp    111  portmapper
    100000    3   tcp    111  portmapper
    100000    2   tcp    111  portmapper
    100000    4   udp    111  portmapper
    100000    3   udp    111  portmapper
    100000    2   udp    111  portmapper
          ... ...
    100007    2   udp    793  ypbind
    100007    1   udp    793  ypbind
    100007    2   tcp    796  ypbind
    100007    1   tcp    796  ypbind
 600100069    1   udp    693  fypxfrd
 600100069    1   tcp    695  fypxfrd
    100009    1   udp   1012  yppasswdd
    100004    2   udp    916  ypserv
    100004    1   udp    916  ypserv
    100004    2   tcp    919  ypserv
    100004    1   tcp    919  ypserv
```

在k8s下的slurm client中：
```bash
root@slurm-client:/#  rpcinfo -p localhost
   program vers proto   port  service
    100000    4   tcp    111  portmapper
    100000    3   tcp    111  portmapper
    100000    2   tcp    111  portmapper
    100000    4   udp    111  portmapper
    100000    3   udp    111  portmapper
    100000    2   udp    111  portmapper
```


## 问题 & solution

```bash
export PATH=/usr/lib64/yp:$PATH

# 查看 docker 
[root@CSBig bin]# dnsdomainname
cluster0.tech-pi.com
[root@CSBig bin]# domainname
cluster0.tech-pi.com

# 启动新镜像
sudo docker run -v /sys/fs/cgroup:/sys/fs/cgroup:ro \
                -d --privileged=true \
                --net=host \
                -it hub.tek-pi.com/env/nis-server:v1.3.1

# 进入容器，执行
/usr/lib64/yp/ypinit -m
```

### slurm 问题描述
client, control, worker用的是同一个image。但是只用ctrl没有yptest命令。

client运行在docker下时没有问题。运行在pod下时：
```bash
[hqlabadmin@CSBig slurm]$ kubectl exec -it slurm-client bash
Runing startup script...
 * Stopping MUNGE munged                                                 [ OK ]
 * Starting MUNGE munged                                                 [ OK ]
root@slurm-client:/# yptest
Test 1: domainname
Configured domainname is "cluster0.tech-pi.com"

Test 2: ypbind
Can't communicate with ypbind
```

考虑到docker下的slurm client启动命令是：


```bash
sudo docker run \
--restart=always \
--name=slurm-client \
-v /mnt/gluster/k8s-pv/slurm-config:/etc/slurm-llnl \
-v /mnt/nfs/spack:/opt/spack \
-v /mnt/gluster/k8s-pv/data:/home/data \
--net=host \
-i -d -t \
hub.tek-pi.com/env/slurm-client
```

与pod的不同之处只有--net=host.

### k8s 网络 explained：
docker 默认提供了四种模式，供容器启动的时候选择：bridge、none、container、host。
kubernetes 的 pod 是用 container模式实现的，同一个 pod 中的容器共享一个 network namespace。

#### container 模式下：

我们会运行两个 nginx 容器：web1 和 web2：

* web1 监听在 80 端口，使用默认的网络模型
* web2 坚挺在 8080 端口，使用 container 网络模型共享 web1 的网络

先启动 web1，通过 port mapping 把端口绑定到主机上：
```bash
$ docker run -d --name=web1 -p 80:80 nginx

$ docker exec web1 ip addr
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
21: eth0@if22: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc noqueue state UP group default
    link/ether 02:42:ac:11:00:02 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.2/16 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::42:acff:fe11:2/64 scope link
       valid_lft forever preferred_lft forever


# 第二个容器
$ docker run --name=web2 \
           -v ${PWD}/default.conf:/etc/nginx/sites-available/default \
           -v ${PWD}/index.html:/var/www/html/index.html \
           --net=container:web1 \ # 这里container模式使用了web1的网络
           -d -p 8080:8080 nginx

docker: Error response from daemon: Conflicting options: port publishing and the container type network mode.
See 'docker run --help'.
```
这里提示端口转发不能和 container 模式一起使用，删除 -p 8080:8080 重新运行，一起正常。但是无法从主机上直接访问 web2 的服务，因为服务端口没有被转发出来。

为什么 web2 容器不允许端口转发呢？当然出于安全和权限控制的角度，不然的话，一个正在运行的容器网络配置可以被任意一个容器修改，这是很危险的行为。那怎么把 web2 的服务转发出来呢？答案是通过 web1：因为它们两个的网络是共享的。我们需要在运行 web1 的时候就指定好所有要映射出来的端口。

那么这种网络模式的应用场景是什么呢？我们还是从特性出发：两个或多个容器共享一个网络空间，包括 ip 和端口等信息。可以类比这种模型到传统虚拟机上面运行的多个应用：多个服务，但是它们共享机器的网络。比如：有一个应用提供 API，另一个应用是它的命令行工具，那么后者可以共享前者的网络。用户只需要访问命令行工具，不用直接和 HTTP 打交道。



sudo docker run --name=slurm-client -v /mnt/gluster/k8s-pv/slurm-config:/etc/slurm-llnl -v /mnt/nfs/spack:/opt/spack -v /mnt/gluster/k8s-pv/data:/home/data -it --rm hub.tek-pi.com/env/slurm-client


### 检查 rpcbind 、ypbind
```bash
# host 下
$ rpcinfo -u localhost ypserv

program 100004 version 1 ready and waiting
program 100004 version 2 ready and waiting

# pod 下
$ rpcinfo -u localhost ypserv

localhost: RPC: Program not registered
```

用 ypbind -debug 查找错误：

```bash
# host 下
$ sudo ypbind -debug

48180: parsing config file
48180: Trying entry: domain cluster0.tech-pi.com server nis.cluster0.tech-pi.com
48180: parsed domain 'cluster0.tech-pi.com' server 'nis.cluster0.tech-pi.com'
48180: add_server() domain: cluster0.tech-pi.com, host: nis.cluster0.tech-pi.com, slot: 0
48180: [Welcome to ypbind-mt, version 1.37.1]

48180: ping interval is 20 seconds

48180: rebind interval is 900 seconds

48181: ypbind-mt already running (pid 10785) - exiting

$ cat /etc/yp.conf
# /etc/yp.conf - ypbind configuration file
# Valid entries are
#
# domain NISDOMAIN server HOSTNAME
#	Use server HOSTNAME for the domain NISDOMAIN.
#
# domain NISDOMAIN broadcast
#	Use  broadcast  on  the local net for domain NISDOMAIN
#
# domain NISDOMAIN slp
#	Query local SLP server for ypserver supporting NISDOMAIN
#
# ypserver HOSTNAME
#	Use server HOSTNAME for the  local  domain.  The
#	IP-address of server must be listed in /etc/hosts.
#
# broadcast
#	If no server for the default domain is specified or
#	none of them is rechable, try a broadcast call to
#	find a server.
#
domain cluster0.tek-pi.com server nis.cluster0.tek-pi.com 


# pod 下
$ ypbind -debug
1861: parsing config file
1861: Trying entry: host: 192.168.1.185
1861: Entry "host: 192.168.1.185" is not valid, ignore it!
1861: No entry found.
No NIS server and no -broadcast option specified.
Add a NIS server to the /etc/yp.conf configuration file,
or start ypbind with the -broadcast option.
```


### systemctl
```bash
systemctl list-unit-files
```

### Reference
[鸟哥linux](http://cn.linux.vbird.org/linux_server/0430nis.php#whatisnis_process)